---
title: "STAT 577 Project"
output: html_notebook
---

# Predicting Sales Using Linear Regression
## A Case Study
### By
## Roshanak Akram
## Nooshin Hamidian
## Parker McCullough
## Varisara Tansakul
### 20 April 2017

```{r}
# Preliminaries
# Data was preprocessed using python with Pandas library, see code/XXX.py
# Load the data from CSV files, Change to absolute path of data
setwd("/Users/roshanak/Dropbox/UTK/Fall2016/STAT576/HW3/Journey/Journey_CSV")
cust = read.csv(file="/Users/roshanak/Dropbox/UTK/Fall2016/STAT576/Homework/HW3/Journey/Journey_CSV/transaction_data.csv", header=TRUE)
demo = read.csv(file="/Users/roshanak/Dropbox/UTK/Spring 2017/STAT 577/Project/demo_ToCategorical.csv", header=TRUE)

head(cust)
summary(cust)

library(sqldf)
stmt <- "SELECT household_key,DAY,STORE_ID,RETAIL_DISC,TRANS_TIME,WEEK_NO,COUPON_DISC,
COUPON_MATCH_DISC,
sum(SALES_VALUE) AS TotalValue
FROM cust
GROUP BY household_key"
trans1 <- sqldf(stmt)

DF = merge(demo, trans1, ,by="household_key")
DF$TRANS_TIME_Cat <- cut(DF$TRANS_TIME,
                     breaks=c(56, 1200, 1201, 2354),
                     labels=c("1","2","3"))
write.csv(DF,file="DF.csv")

#####Just Using the prepared data
#DF = read.csv(file="/Users/roshanak/Dropbox/UTK/Spring 2017/STAT 577/Project/DF.csv", header = TRUE)
#DF= DF[,-1]
#plot(DFs,pch=16,cex=.5) 

#directly fit least squares
Y = as.numeric(DF[,16])

#Fit linear model using "lm"
fit = lm(Y~.,data=DF)
summary(fit)
residuals = fit$residuals

#Test for Autocorrelated Errors
durbinWatsonTest(fit)

# variance inflation factor for collinearity
vif(fit)
sqrt(vif(fit)) > 3  # problem ( or may be even take value to be 2)

# Non-constant error variance test
ncvTest(fit)

set.seed(100)
train_index = sample(1:nrow(DF),0.5*nrow(DF)) 
test_data_ols = DF[-train_index,] 

# fitted models
fit_ols = lm(TotalValue ~ .,data = DF[train_index,]) 

# Obtain Predicted values
train_pred_ols = fit_ols$fitted.values
test_pred_ols = predict(fit_ols,newdata = test_data_ols) 

# Compute MSEs
train_mse_ols = mean((DF[train_index,16]-train_pred_ols)^2)
test_mse_ols = mean((DF[-train_index,16]-test_pred_ols)^2)


c(train_mse_ols) # print training errors
c(test_mse_ols) # print training errors

##Trying different methods:
#prostate = read.csv("prostate.csv")
#colnames(prostate) = c("lcavol","lweight","age","lbph","svi","lcp","gleason","pgg45","lpsa")
#head(prostate)
DF = na.omit(DF)
DF$household_key = as.numeric(DF$household_key)
DF$AGE_DESC = as.numeric(DF$AGE_DESC)
DF$MARITAL_STATUS_CODE = as.numeric(DF$MARITAL_STATUS_CODE)
DF$INCOME_DESC = as.numeric(DF$INCOME_DESC)
DF$HOMEOWNER_DESC = as.numeric(DF$HOMEOWNER_DESC)
DF$HH_COMP_DESC = as.numeric(DF$HH_COMP_DESC)
DF$HOUSEHOLD_SIZE_DESC = as.numeric(DF$HOUSEHOLD_SIZE_DESC)
DF$KID_CATEGORY_DESC = as.numeric(DF$KID_CATEGORY_DESC)
DF$DAY = as.numeric(DF$DAY)
DF$STORE_ID = as.numeric(DF$STORE_ID)
DF$RETAIL_DISC = as.numeric(DF$RETAIL_DISC)
DF$TRANS_TIME = as.numeric(DF$TRANS_TIME)
DF$WEEK_NO = as.numeric(DF$WEEK_NO)
DF$TRANS_TIME_Cat =  as.numeric(DF$TRANS_TIME_Cat)
str(DF) 
M = cor(DF)
corrplot(M, method = "circle")



YY = as.numeric(DF[,16]); YY = YY - mean(YY)  # center the response
XX = as.matrix(DF[,-16]); XX = scale(XX,center=T,scale=T) # center the predictors
n = nrow(XX)
rep = 20
mse.train.lm = mse.train.bs = mse.train.f = mse.train.b = mse.train.l = mse.train.r = mse.train.en = mse.train.pcr = mse.train.pls = rep(0,rep)
mse.test.lm = mse.test.bs = mse.test.f = mse.test.b = mse.test.l = mse.test.r = mse.test.en =
mse.test.pcr = mse.test.pls = rep(0,rep)

# loop
set.seed(1)
for(i in 1:rep)
{
  train_index = sample(1:n,n/2,replace = FALSE)  # randomly select the indices of the training set
  X = XX[train_index,]  # trainining X
  Y = YY[train_index]  # training Y
  X_test = XX[-c(train_index),] # Test X
  Y_test = YY[-c(train_index)] # Test Y
  train_data = data.frame(X,Y)
  test_data = data.frame(X_test,Y_test)
  
  # LM
  lm.fit = lm(Y~., data = train_data) #
  pred.train.lm = predict(lm.fit)
  pred.test.lm = predict(lm.fit,newdata = test_data[,-17])
  mse.train.lm[i] = sum((Y - pred.train.lm)^2)/length(Y)
  mse.test.lm[i] = sum((Y_test - pred.test.lm)^2)/length(Y_test)
  
  #Best Subset
  fitbsub = regsubsets(x=X,y=Y,nbest = 1, nvmax = 16)
  rs = summary(fitbsub)
  plot(rs$bic, xlab="Parameter", ylab="BIC",type = 'l')
  bs.fit = lm(Y ~ INCOME_DESC+HH_COMP_DESC+DAY+WEEK_NO,data=train_data )#!!!!!!!
  pred.train.bs = predict(bs.fit)
  pred.test.bs = predict(bs.fit,newdata = test_data[,c(4,6,9,13)])#!!!!!!!
  mse.train.bs[i] = sum((Y - pred.train.bs)^2)/length(Y)
  mse.test.bs[i] = sum((Y_test - pred.test.bs)^2)/length(Y_test)
  
  # forward step-wise
  #fitf0 = lm(Y~1,data=train_data) # fit the model with only the intercept
  #fitf = stepAIC(fitf0,lpsa~.,direction="forward",data=prostate,k=log(nrow(prostate)))
  #pred.train.f = predict(fitf)
  #pred.train.f = predict(fitf,newdata = test_data[,-9])
  #mse.train.f[i] = sum(Y - pred.train.f)^2/length(Y)
  #mse.test.f[i] = sum(Y_test - pred.test.f)^2/length(Y_test)
  
  #backward step-wise
  fitb0 = lm(Y~.,data=train_data)
  fitb = stepAIC(fitb0,direction="backward",data=DF,k=log(nrow(DF)),trace=FALSE)
  pred.train.b  = predict(fitb)
  pred.test.b = predict(fitb,newdata = test_data[,-17])
  mse.train.b[i] = sum((Y - pred.train.b)^2)/length(Y)
  mse.test.b[i] = sum((Y_test - pred.test.b)^2)/length(Y_test)
  
  #lasso
  cvfitl = cv.glmnet(x=X,y=Y,family="gaussian",alpha=1,standardize=FALSE)
  pred.train.l = predict(cvfitl, newx = X, s = "lambda.min") 
  pred.test.l = predict(cvfitl, newx = X_test, s = "lambda.min") 
  mse.train.l[i] = sum((Y - pred.train.l)^2)/length(Y)
  mse.test.l[i] = sum((Y_test - pred.test.l)^2)/length(Y_test)
  
  # Ridge
  cvfitr = cv.glmnet(x=X,y=Y,family="gaussian",alpha=0,standardize=FALSE)
  pred.train.r = predict(cvfitr, newx = X, s = "lambda.min") 
  pred.test.r = predict(cvfitr, newx = X_test, s = "lambda.min") 
  mse.train.r[i] = sum((Y - pred.train.r)^2)/length(Y)
  mse.test.r[i] = sum((Y_test - pred.test.r)^2)/length(Y_test)
  
  # E - net
  cvfiten = cv.glmnet(x=X,y=Y,family="gaussian",alpha=0.5,standardize=FALSE)
  pred.train.en = predict(cvfiten, newx = X, s = "lambda.min") 
  pred.test.en = predict(cvfiten, newx = X_test, s = "lambda.min") 
  mse.train.en[i] = sum((Y - pred.train.en)^2)/length(Y)
  mse.test.en[i] = sum((Y_test - pred.test.en)^2)/length(Y_test)
  
  # PC regression 
  pcr.fit=pcr(Y~X,scale=TRUE,ncomp = 5)
  pred.train.pcr = predict(pcr.fit, ncomp = 5, newdata = X)
  pred.test.pcr = predict(pcr.fit, ncomp = 5, newdata = X_test)
  mse.train.pcr[i] = sum((Y - pred.train.pcr)^2)/length(Y)
  mse.test.pcr[i] = sum((Y_test - pred.test.pcr)^2)/length(Y_test)
  
  # PLS regression
  pls.fit=plsr(Y~X,scale=TRUE,ncomp = 5)
  pred.train.pls = predict(pls.fit, ncomp = 5, newdata = X)
  pred.test.pls = predict(pls.fit, ncomp = 5, newdata = X_test)
  mse.train.pls[i] = sum((Y - pred.train.pls)^2)/length(Y)
  mse.test.pls[i] = sum((Y_test - pred.test.pls)^2)/length(Y_test)
}

mse.train = c(mse.train.lm,mse.train.bs, mse.train.b, mse.train.l,mse.train.r,mse.train.en,
                mse.train.pcr,mse.train.pls)
mse.test = c(mse.test.lm, mse.test.bs, mse.test.b, mse.test.l, mse.test.r, mse.test.en, 
             mse.test.pcr, mse.test.pls)
indicator = c(rep("LM",rep),rep("BS",rep),rep("BackWard",rep),rep("Lasso",rep),rep("Ridge",rep),
              rep("ENET",rep),rep("PCR",rep),rep("PLSR",rep))
boxplot(mse.train~as.factor(indicator),main="Training MSE",ylab="MSE")
boxplot(mse.test~as.factor(indicator),main="Test MSE",ylab="MSE")

# How do we know the methods are doing well?
# lets look at the scatter plots of the test predicted values and the actual values

test_preds = as.data.frame(cbind(Y_test,pred.test.lm,pred.test.bs,pred.test.b,pred.test.l,pred.test.r
                   ,pred.test.en, pred.test.pcr,pred.test.pls))
names(test_preds)=c("Test Actual","LM Preds","BS Preds","Backward Preds",
                    "Lasso Preds","Ridge Preds","ENET Preds","PCR Preds",
                    "PLSR Preds")
plot(test_preds,pch=16)

# Estimated Coefficient Vectors
beta.lm = lm.fit$coefficients[-1]
beta.r= coef(cvfitr, s = "lambda.min")[-1]
beta.l = coef(cvfitl, s = "lambda.min")[-1]
beta.en = coef(cvfiten, s = "lambda.min")[-1]

coefs = cbind(beta.lm,beta.r,beta.l,beta.en)
```


